{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2ab0f4c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-12T07:06:14.975379Z",
     "iopub.status.busy": "2025-06-12T07:06:14.975079Z",
     "iopub.status.idle": "2025-06-12T07:06:39.164183Z",
     "shell.execute_reply": "2025-06-12T07:06:39.163088Z"
    },
    "papermill": {
     "duration": 24.194844,
     "end_time": "2025-06-12T07:06:39.166056",
     "exception": false,
     "start_time": "2025-06-12T07:06:14.971212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDVI Land Cover Classification\n",
      "==================================================\n",
      "Loading training data...\n",
      "Training data shape: (8000, 30)\n",
      "Loading test data...\n",
      "Test data shape: (2845, 29)\n",
      "\n",
      "Class distribution in training data:\n",
      "class\n",
      "forest        6159\n",
      "farm           841\n",
      "impervious     669\n",
      "grass          196\n",
      "water          105\n",
      "orchard         30\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample NDVI columns:\n",
      "Found 27 NDVI time points\n",
      "First few NDVI columns: ['20150720_N', '20150602_N', '20150517_N', '20150501_N', '20150415_N']\n",
      "\n",
      "Missing values in training data: 25040\n",
      "Missing values in test data: 0\n",
      "Starting training process...\n",
      "Extracting temporal features...\n",
      "Handling missing values...\n",
      "Scaling features...\n",
      "Training logistic regression model...\n",
      "Cross-validation accuracy: 0.8946 (+/- 0.0411)\n",
      "Making predictions...\n",
      "Extracting temporal features...\n",
      "Handling missing values...\n",
      "Scaling features...\n",
      "Submission saved to submission.csv\n",
      "Submission shape: (2845, 2)\n",
      "Class distribution in predictions:\n",
      "class\n",
      "forest        1656\n",
      "farm           512\n",
      "impervious     391\n",
      "grass          182\n",
      "water          103\n",
      "orchard          1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==================================================\n",
      "SUCCESS: Model trained and submission file created!\n",
      "File: submission.csv\n",
      "Format: ID,class\n",
      "Ready for Kaggle submission!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class NDVILandCoverClassifier:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.imputer = SimpleImputer(strategy='mean')\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.model = LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            multi_class='ovr',\n",
    "            random_state=42,\n",
    "            C=1.0\n",
    "        )\n",
    "        self.feature_columns = []\n",
    "        \n",
    "    def extract_temporal_features(self, df):\n",
    "        \"\"\"Extract meaningful features from NDVI time series\"\"\"\n",
    "        # Get NDVI columns (assuming they end with '_N')\n",
    "        ndvi_cols = [col for col in df.columns if col.endswith('_N')]\n",
    "        \n",
    "        # Store original NDVI data\n",
    "        ndvi_data = df[ndvi_cols].copy()\n",
    "        \n",
    "        # Create feature dataframe\n",
    "        features = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        # Basic statistical features\n",
    "        features['ndvi_mean'] = ndvi_data.mean(axis=1)\n",
    "        features['ndvi_std'] = ndvi_data.std(axis=1)\n",
    "        features['ndvi_min'] = ndvi_data.min(axis=1)\n",
    "        features['ndvi_max'] = ndvi_data.max(axis=1)\n",
    "        features['ndvi_range'] = features['ndvi_max'] - features['ndvi_min']\n",
    "        features['ndvi_median'] = ndvi_data.median(axis=1)\n",
    "        \n",
    "        # Percentiles\n",
    "        features['ndvi_25th'] = ndvi_data.quantile(0.25, axis=1)\n",
    "        features['ndvi_75th'] = ndvi_data.quantile(0.75, axis=1)\n",
    "        features['ndvi_iqr'] = features['ndvi_75th'] - features['ndvi_25th']\n",
    "        \n",
    "        # Temporal trend features\n",
    "        features['ndvi_trend'] = ndvi_data.apply(self._calculate_trend, axis=1)\n",
    "        features['ndvi_seasonality'] = ndvi_data.apply(self._calculate_seasonality, axis=1)\n",
    "        \n",
    "        # Growing season features (assume peak growing season is middle of year)\n",
    "        mid_year_cols = ndvi_cols[len(ndvi_cols)//3:2*len(ndvi_cols)//3]\n",
    "        if mid_year_cols:\n",
    "            features['growing_season_mean'] = df[mid_year_cols].mean(axis=1)\n",
    "            features['growing_season_max'] = df[mid_year_cols].max(axis=1)\n",
    "        \n",
    "        # Vegetation vigor (high NDVI values indicate healthy vegetation)\n",
    "        features['high_ndvi_count'] = (ndvi_data > 0.5).sum(axis=1)\n",
    "        features['low_ndvi_count'] = (ndvi_data < 0.2).sum(axis=1)\n",
    "        \n",
    "        # Missing data indicators\n",
    "        features['missing_count'] = ndvi_data.isnull().sum(axis=1)\n",
    "        features['missing_ratio'] = features['missing_count'] / len(ndvi_cols)\n",
    "        \n",
    "        # Add original NDVI values (important for logistic regression)\n",
    "        for col in ndvi_cols:\n",
    "            features[f'original_{col}'] = df[col]\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _calculate_trend(self, series):\n",
    "        \"\"\"Calculate linear trend of NDVI values\"\"\"\n",
    "        valid_data = series.dropna()\n",
    "        if len(valid_data) < 2:\n",
    "            return 0\n",
    "        \n",
    "        x = np.arange(len(valid_data))\n",
    "        try:\n",
    "            slope = np.polyfit(x, valid_data.values, 1)[0]\n",
    "            return slope\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def _calculate_seasonality(self, series):\n",
    "        \"\"\"Calculate measure of seasonality\"\"\"\n",
    "        valid_data = series.dropna()\n",
    "        if len(valid_data) < 3:\n",
    "            return 0\n",
    "        \n",
    "        # Simple seasonality measure: coefficient of variation\n",
    "        return valid_data.std() / (valid_data.mean() + 1e-8)\n",
    "    \n",
    "    def preprocess_data(self, df, is_training=True):\n",
    "        \"\"\"Preprocess the data with feature engineering\"\"\"\n",
    "        print(\"Extracting temporal features...\")\n",
    "        features = self.extract_temporal_features(df)\n",
    "        \n",
    "        print(\"Handling missing values...\")\n",
    "        # Impute missing values\n",
    "        if is_training:\n",
    "            features_imputed = pd.DataFrame(\n",
    "                self.imputer.fit_transform(features),\n",
    "                columns=features.columns,\n",
    "                index=features.index\n",
    "            )\n",
    "        else:\n",
    "            features_imputed = pd.DataFrame(\n",
    "                self.imputer.transform(features),\n",
    "                columns=features.columns,\n",
    "                index=features.index\n",
    "            )\n",
    "        \n",
    "        # Store feature columns\n",
    "        if is_training:\n",
    "            self.feature_columns = features_imputed.columns.tolist()\n",
    "        \n",
    "        print(\"Scaling features...\")\n",
    "        # Scale features\n",
    "        if is_training:\n",
    "            features_scaled = pd.DataFrame(\n",
    "                self.scaler.fit_transform(features_imputed),\n",
    "                columns=features_imputed.columns,\n",
    "                index=features_imputed.index\n",
    "            )\n",
    "        else:\n",
    "            features_scaled = pd.DataFrame(\n",
    "                self.scaler.transform(features_imputed),\n",
    "                columns=features_imputed.columns,\n",
    "                index=features_imputed.index\n",
    "            )\n",
    "        \n",
    "        return features_scaled\n",
    "    \n",
    "    def train(self, train_df):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        print(\"Starting training process...\")\n",
    "        \n",
    "        # Separate features and target\n",
    "        X = train_df.drop(['class', 'ID'], axis=1, errors='ignore')\n",
    "        y = train_df['class']\n",
    "        \n",
    "        # Encode labels\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        \n",
    "        # Preprocess features\n",
    "        X_processed = self.preprocess_data(X, is_training=True)\n",
    "        \n",
    "        # Train model\n",
    "        print(\"Training logistic regression model...\")\n",
    "        self.model.fit(X_processed, y_encoded)\n",
    "        \n",
    "        # Cross-validation score\n",
    "        cv_scores = cross_val_score(self.model, X_processed, y_encoded, cv=5, scoring='accuracy')\n",
    "        print(f\"Cross-validation accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, test_df):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        print(\"Making predictions...\")\n",
    "        \n",
    "        # Preprocess features\n",
    "        X_test = test_df.drop(['ID'], axis=1, errors='ignore')\n",
    "        X_processed = self.preprocess_data(X_test, is_training=False)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred_encoded = self.model.predict(X_processed)\n",
    "        y_pred = self.label_encoder.inverse_transform(y_pred_encoded)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def create_submission(self, test_df, predictions, filename='submission.csv'):\n",
    "        \"\"\"Create submission file\"\"\"\n",
    "        submission = pd.DataFrame({\n",
    "            'ID': test_df['ID'],\n",
    "            'class': predictions\n",
    "        })\n",
    "        \n",
    "        # Ensure proper formatting\n",
    "        submission['class'] = submission['class'].str.lower()\n",
    "        \n",
    "        submission.to_csv(filename, index=False)\n",
    "        print(f\"Submission saved to {filename}\")\n",
    "        print(f\"Submission shape: {submission.shape}\")\n",
    "        print(\"Class distribution in predictions:\")\n",
    "        print(submission['class'].value_counts())\n",
    "        \n",
    "        return submission\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"NDVI Land Cover Classification\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize classifier\n",
    "    classifier = NDVILandCoverClassifier()\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        print(\"Loading training data...\")\n",
    "        train_df = pd.read_csv('/kaggle/input/summer-analytics-mid-hackathon/hacktrain.csv')\n",
    "        print(f\"Training data shape: {train_df.shape}\")\n",
    "        \n",
    "        print(\"Loading test data...\")\n",
    "        test_df = pd.read_csv('/kaggle/input/summer-analytics-mid-hackathon/hacktest.csv')\n",
    "        print(f\"Test data shape: {test_df.shape}\")\n",
    "        \n",
    "        # Display basic info\n",
    "        print(\"\\nClass distribution in training data:\")\n",
    "        print(train_df['class'].value_counts())\n",
    "        \n",
    "        print(\"\\nSample NDVI columns:\")\n",
    "        ndvi_cols = [col for col in train_df.columns if col.endswith('_N')]\n",
    "        print(f\"Found {len(ndvi_cols)} NDVI time points\")\n",
    "        print(\"First few NDVI columns:\", ndvi_cols[:5])\n",
    "        \n",
    "        # Check for missing values\n",
    "        print(f\"\\nMissing values in training data: {train_df.isnull().sum().sum()}\")\n",
    "        print(f\"Missing values in test data: {test_df.isnull().sum().sum()}\")\n",
    "        \n",
    "        # Train model\n",
    "        classifier.train(train_df)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = classifier.predict(test_df)\n",
    "        \n",
    "        # Create submission\n",
    "        submission = classifier.create_submission(test_df, predictions, 'submission.csv')\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"SUCCESS: Model trained and submission file created!\")\n",
    "        print(\"File: submission.csv\")\n",
    "        print(\"Format: ID,class\")\n",
    "        print(\"Ready for Kaggle submission!\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Could not find data files. Please ensure 'train.csv' and 'test.csv' are in the current directory.\")\n",
    "        print(\"Expected files:\")\n",
    "        print(\"- train.csv (with columns: ID, class, and NDVI time series)\")\n",
    "        print(\"- test.csv (with columns: ID and NDVI time series)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        print(\"Please check your data format and try again.\")\n",
    "\n",
    "# Alternative function for custom data loading\n",
    "def train_and_predict_custom(train_path, test_path, submission_path='submission.csv'):\n",
    "    \"\"\"\n",
    "    Custom function for different file paths\n",
    "    \"\"\"\n",
    "    classifier = NDVILandCoverClassifier()\n",
    "    \n",
    "    # Load data\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    \n",
    "    # Train and predict\n",
    "    classifier.train(train_df)\n",
    "    predictions = classifier.predict(test_df)\n",
    "    \n",
    "    # Create submission\n",
    "    submission = classifier.create_submission(test_df, predictions, submission_path)\n",
    "    \n",
    "    return submission\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12585144,
     "sourceId": 104491,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30.172757,
   "end_time": "2025-06-12T07:06:39.889839",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-12T07:06:09.717082",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
